# Web Workers + Perturbation Theory: Architecture Discussion

**Date:** 2025-11-16
**Status:** Research & Design Phase
**Context:** Deep dive into progressive rendering architecture for 30-minute deep zoom renders

---

## Executive Summary

This document captures our architectural exploration of implementing Web Workers for progressive rendering in Fractal Wonder, with particular focus on perturbation theory requirements for deep zoom (10^100+) rendering.

**Key findings:**
- Progressive rendering is essential (30-minute renders make it non-negotiable)
- Manual Web Workers required (wasm-bindgen-rayon blocks until completion)
- Perturbation theory requires adaptive quadtree subdivision (not fixed tile grid)
- Architecture complexity increases significantly with perturbation theory

**Critical decision pending:** MVP without perturbation vs. designing for perturbation from start

---

We want our app to achieve the following things:

- user interaction should always be responsive
- we want to use as much CPU power as possible, across multiple cores
- the workers ONLY compute data per pixel. The main thread will colorise and put on canvas
- panning and zooming should immediately stop computation and prevent runaway compute threads
- a strong candidate for the multi threading mechanism is wasm-bindgen-rayon + SharedArrayBuffer
- rendering **must** be progressive: the user should see progress in some form while the fractal is rendering.
- we should not use tiles.par_iter().map(compute).collect() because that is incompatible with progressive rendering
- instead tile computation should write directly to the shared buffer, e.g. something like:

    ```txt
    tiles.par_iter().for_each(|tile| {
        render_tile_into_shared_buffer(tile);
        mark_tile_ready(tile.index);
    });
    ```

For the fractal computation itself:

- we want to support EXTREME zoom levels (e.g. 10^100)
- we want to implement perturbation theory in the future
- for perturbation theory we will need an adaptive quad tree tiling algorithm:
  - initially we compute one reference orbit for the viewport (likely its center)
  - we compute the four corner points as well
  - if the results are unstable, we subdivide the tile and recursively apply the same algorithm
  - once we have stable reference orbits, we us perturbation theory to compute the tile's pixels
- we already support arbitrary precision (used for reference orbit computation)

RESEARCH QUESTIONS:

- RQ-1: Using perturbation theory, at extreme zoom levels, can we use 64 bit precision for all computation apart from
  the reference orbits?
- RQ-2: Could we later swap out (or add to) the CPU web workers and use GPU instead for all computation apart from
  reference orbits?
- RQ-3: How do we quickly stop all compute when the user wants to interact with the app?
- RQ-4: I had issues in the past where workers could not run because the WASM had a dependency on the DOM via Leptos
  and hydrate(). Is this an issue with wasm-bindgen-rayon? Do we need multiple entry points (worker / main), multiple
  crates, or something else?
- if we need multiple crates or WASM files, can we still use `trunk serve` or implement another way to watch files
  and rebuild on the fly during development?
- RQ-6: where should the reference orbits be computed? On the worker threads? Or on the main thread?
- RQ-7: Describe a high level algorithm that implements the above

---

## Research Answers

### RQ-1: Precision Requirements with Perturbation Theory

**Can we use 64-bit precision for all computation except reference orbits at extreme zoom levels?**

**Answer: Yes. This is the primary benefit of perturbation theory.**

Standard 64-bit double precision provides approximately 15-16 decimal digits of precision. Without perturbation theory, zoom levels become blocky around 10^-16 because adjacent pixels cannot be represented with distinct values.

Perturbation theory solves this by:
1. Computing ONE reference orbit using arbitrary precision (our `dashu` crate)
2. Computing ALL other pixels using 64-bit `f64` to store the delta from the reference orbit
3. Using the formula: `Δₙ₊₁ = 2Xₙ·Δₙ + Δₙ² + Δ₀` where all Δ values are `f64`

This achieves ~512x speedup compared to using arbitrary precision for every pixel. You can reach zoom levels beyond 10^100 while computing 99.9%+ of calculations in `f64`. Only reference orbit computation and coordinate transformations require arbitrary precision.

**Critical caveat:** Multiple reference orbits become necessary when perturbations become unstable (large deviation, deep zooms, orbit derivatives blowing up, long escape times).

### RQ-2: GPU Feasibility for Perturbation Calculations

**Can we swap CPU web workers for GPU, excluding reference orbits?**

**Answer: Yes, with caveats. GPU acceleration works for perturbation-based fractal rendering.**

Perturbation calculations use `f64` operations (delta computations). Each pixel's calculation is independent once you have the reference orbit. This matches GPU's SIMD architecture perfectly.

**Implementation paths:**

**WebGL 2.0 + Fragment Shaders:**
- Available today in browsers
- Limitation: Limited `f64` support (many GPUs only have `f32` in shaders)
- Workaround: Emulated double precision using dual `f32` numbers

**WebGPU (recommended):**
- Better compute shader support
- More flexible memory management
- Growing browser support (Chrome, Edge, Safari)

**Hybrid architecture:**
```
Reference orbits (CPU/arbitrary precision)
    ↓
Adaptive quadtree subdivision (CPU)
    ↓
Perturbation calculations (GPU) ← Swap GPU workers here
    ↓
Coloring/visualization (GPU or CPU)
```

**Critical considerations:**
- Reference orbit computation must stay on CPU (requires arbitrary precision)
- Data transfer between CPU↔GPU has overhead (minimize by batching)
- Progressive rendering becomes trickier (need to copy partial results back)

**Recommendation:** Design CPU Web Workers architecture with abstraction for the compute backend, making GPU a future swap-in option.

### RQ-3: Stopping Computation on User Interaction

**How do we quickly stop all compute when the user interacts with the app?**

**Answer: Use multiple strategies. WebAssembly cannot be interrupted mid-execution.**

The fundamental problem: Once a WASM function runs, it blocks until completion. You cannot send signals like Linux `kill` to WASM code.

**Strategy 1: Cooperative Cancellation (Polling Pattern)**

```rust
pub fn render_tile_with_cancellation(
    tile: &Tile,
    cancel_flag: &AtomicBool,  // SharedArrayBuffer-backed
) -> Option<TileResult> {
    for iteration in 0..max_iterations {
        // Check every N iterations (balance overhead vs responsiveness)
        if iteration % 100 == 0 && cancel_flag.load(Ordering::Relaxed) {
            return None; // Abort computation
        }
        // ... actual computation ...
    }
}
```

Pros: Clean exit, can save partial results
Cons: Completes current iteration chunk before checking

**Strategy 2: Worker Termination (Nuclear Option)**

```javascript
worker.terminate(); // Kills entire worker immediately
// Spawn new worker for next render
```

Pros: Instant stop, guaranteed
Cons: Loses all state, must reinitialize worker (expensive)

**Strategy 3: Chunked Work with Yield Points**

```rust
tiles.par_iter().for_each(|tile| {
    for sub_chunk in tile.chunks() {
        if should_cancel() { return; }
        render_sub_chunk(sub_chunk);
        // Natural yield point between chunks
    }
});
```

**Recommended hybrid approach:**
1. Use SharedArrayBuffer + AtomicBool for cancel flag
2. Poll the flag every ~100 iterations (tune based on performance)
3. Keep worker pool alive between renders (avoid terminating)
4. For unresponsive workers: Track last response time, terminate if stuck >500ms

### RQ-4: WASM DOM Dependencies and Worker Compatibility

**Does the WASM + DOM dependency issue affect wasm-bindgen-rayon? Do we need multiple entry points or crates?**

**Answer: Yes, this is a critical issue. The current architecture will fail in workers.**

Your current problem (confirmed in `src/lib.rs:11`):

```rust
#[wasm_bindgen]
pub fn hydrate() {
    leptos::mount_to_body(|| { // ← DOM dependency!
        view! { <app::App /> }
    });
}
```

This cannot run in a Web Worker (workers have no DOM).

**Solution: Workspace with separate crates**

**Current structure:**
```
fractalwonder/
├── src/
├── tests/
├── public/
├── dist/
├── index.html
├── Cargo.toml
├── Trunk.toml
└── other config files
```

**Proposed workspace structure:**

```
fractalwonder/                          # Root workspace
├── Cargo.toml                          # Workspace manifest (MODIFIED)
├── index.html                          # Root HTML (MODIFIED - references both WASMs)
├── Trunk.toml                          # Trunk config (stays at root)
├── public/                             # Static assets (stays at root)
├── dist/                               # Build output (stays at root)
├── input.css                           # Tailwind CSS (stays at root)
├── package.json, etc.                  # Other configs (stay at root)
│
├── fractalwonder-ui/                   # UI crate (NEW directory)
│   ├── Cargo.toml                      # UI-specific dependencies (Leptos, etc.)
│   └── src/
│       ├── lib.rs                      # Entry point with hydrate()
│       ├── app.rs                      # MOVED from src/app.rs
│       ├── components/                 # MOVED from src/components/
│       ├── hooks/                      # MOVED from src/hooks/
│       └── state/                      # MOVED from src/state/
│
├── fractalwonder-compute/              # Worker crate (NEW directory)
│   ├── Cargo.toml                      # Compute dependencies (NO Leptos!)
│   └── src/
│       ├── lib.rs                      # Worker entry point (NEW)
│       └── worker.rs                   # Worker implementation (NEW)
│
├── fractalwonder-core/                 # Shared core (NEW directory)
│   ├── Cargo.toml                      # Core dependencies (dashu, etc.)
│   └── src/
│       ├── lib.rs                      # Re-exports
│       └── rendering/                  # MOVED from src/rendering/
│           ├── coords.rs
│           ├── mandelbrot.rs
│           └── etc.
│
└── tests/                              # Integration tests (stays at root)
    └── wasm.rs
```

**Key file changes:**

**Root `Cargo.toml` (becomes workspace manifest):**
```toml
[workspace]
members = [
    "fractalwonder-ui",
    "fractalwonder-compute",
    "fractalwonder-core",
]
resolver = "2"

[workspace.dependencies]
fractalwonder-core = { path = "./fractalwonder-core" }
dashu = "0.4"
dashu-float = "0.4"
wasm-bindgen = "0.2"
serde = { version = "1.0", features = ["derive"] }
# ... other shared dependencies
```

**Root `index.html` (references both WASMs):**
```html
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Fractal Wonder</title>

    <!-- Main UI WASM -->
    <link data-trunk rel="rust" href="./fractalwonder-ui/Cargo.toml"/>

    <!-- Worker WASM -->
    <link data-trunk rel="rust" data-type="worker"
          href="./fractalwonder-compute/Cargo.toml"/>

    <!-- Tailwind CSS -->
    <link data-trunk rel="tailwind-css" href="./input.css"/>
</head>
<body></body>
</html>
```

**`fractalwonder-ui/Cargo.toml`:**
```toml
[package]
name = "fractalwonder-ui"
version = "0.1.0"
edition = "2021"

[lib]
crate-type = ["cdylib", "rlib"]

[dependencies]
fractalwonder-core.workspace = true
leptos = { version = "0.6", features = ["csr"] }
wasm-bindgen.workspace = true
leptos-use = "0.13"
web-sys = { version = "0.3", features = [...] }
# ... other UI dependencies
```

**`fractalwonder-compute/Cargo.toml`:**
```toml
[package]
name = "fractalwonder-compute"
version = "0.1.0"
edition = "2021"

[lib]
crate-type = ["cdylib"]

[dependencies]
fractalwonder-core.workspace = true
wasm-bindgen.workspace = true
wasm-bindgen-rayon = "1.2"
js-sys = "0.3"
# NO Leptos here!
# NO web-sys DOM features here!
```

**`fractalwonder-core/Cargo.toml`:**
```toml
[package]
name = "fractalwonder-core"
version = "0.1.0"
edition = "2021"

[dependencies]
dashu.workspace = true
dashu-float.workspace = true
serde.workspace = true
dyn-clone = "1.0.20"
# Pure computation, no UI dependencies
```

**wasm-bindgen-rayon consideration:**

Documentation states: "The main thread still can't be blocked while waiting for the Rayon pool to get ready, requiring you to instantiate the main JS+Wasm in a dedicated Worker."

This means:
1. Main thread → UI only (Leptos)
2. Dedicated "main compute" worker → Instantiates your WASM with rayon
3. Additional rayon workers → Spawned by wasm-bindgen-rayon

Your `hydrate()` function cannot be in the worker WASM.

**Migration steps:**
1. Create the three new directories
2. Move `src/rendering/` → `fractalwonder-core/src/`
3. Move `src/components/`, `src/hooks/`, `src/state/`, `src/app.rs` → `fractalwonder-ui/src/`
4. Move `src/lib.rs` → `fractalwonder-ui/src/lib.rs`
5. Create new `fractalwonder-compute/src/lib.rs` for worker entry point
6. Update root `Cargo.toml` to workspace manifest
7. Update `index.html` to reference both crates
8. Delete old `src/` directory

**Development workflow stays the same:**
```bash
trunk serve  # Still works from root, watches all workspace members
```

This keeps your root directory clean (public/, dist/, configs stay at root) while properly separating concerns.

### RQ-5: Development Workflow with Multiple WASM Files

**Can we use `trunk serve` with multiple crates/WASM files, or do we need another approach?**

**Answer: Yes. Trunk supports multiple WASM files with configuration.**

Trunk handles multiple WASM binaries using HTML-based asset declarations:

```html
<!-- index.html -->
<!DOCTYPE html>
<html>
<head>
    <!-- Main application WASM -->
    <link data-trunk rel="rust" href="./fractalwonder-ui/Cargo.toml"/>

    <!-- Worker WASM -->
    <link data-trunk rel="rust" data-type="worker"
          href="./fractalwonder-compute/Cargo.toml"/>
</head>
<body></body>
</html>
```

**Workspace structure:**

```toml
# Root Cargo.toml
[workspace]
members = [
    "fractalwonder-ui",
    "fractalwonder-compute",
    "fractalwonder-core",
]

[workspace.dependencies]
fractalwonder-core = { path = "./fractalwonder-core" }
```

**Development workflow:**

```bash
# From project root
trunk serve

# This will:
# 1. Build fractalwonder-ui → main.wasm
# 2. Build fractalwonder-compute → worker.wasm
# 3. Watch both + fractalwonder-core for changes
# 4. Auto-rebuild and reload on save
# 5. Serve at localhost:8080 with CORS headers
```

Trunk watches ALL referenced Cargo.toml files and their dependencies. Saving a file in any workspace member triggers rebuilds of affected crates with automatic browser reload.

**Key features:**
- Use `data-type="worker"` to compile a Rust project as a Web Worker
- The `href` attribute specifies the path to the worker's Cargo.toml
- Trunk compiles it separately and handles bundling
- File watching works across entire workspace

**Note:** Trunk's multi-WASM support is proven (used by Yew's multi_thread example).

### RQ-6: Reference Orbit Computation Location

**Where should reference orbits be computed: worker threads or main thread?**

**Answer: Worker threads (dedicated coordinator worker), NOT the main thread.**

**Why NOT the main thread:**

1. **Blocking computation (30+ minutes)**
   - Reference orbit for extreme zoom can take seconds to minutes
   - Main thread MUST remain responsive (your #1 requirement)
   - Blocking main = UI freeze = terrible UX

2. **Arbitrary precision is expensive**
   - `dashu` arbitrary precision math is slow
   - Reference orbit needs hundreds/thousands of iterations
   - At 10^100 zoom, coordinate precision alone is expensive

3. **Architecture principle violation**
   - Your document states: "workers ONLY compute data per pixel. Main thread colorizes and puts on canvas"
   - Reference orbit computation IS pixel data computation

**Recommended: Dedicated coordinator worker**

```
┌─────────────────┐
│  Main Thread    │  ← UI, canvas, user interaction
│  (Leptos/DOM)   │
└────────┬────────┘
         │
         ↓ postMessage (viewport, zoom)
┌─────────────────────────┐
│  Coordinator Worker     │  ← Reference orbit computation
│  (Single thread)        │  ← Quadtree subdivision logic
│  - Arbitrary precision  │  ← Task distribution
│  - Adaptive tiling      │
└────────┬────────────────┘
         │
         ↓ Distribute tiles + reference orbits
┌─────────────────────────────────────────┐
│  Compute Worker Pool (wasm-bindgen-rayon)│
│  Worker 1 │ Worker 2 │ ... │ Worker N   │
│  ← Perturbation calculations (f64)       │
│  ← Writes to SharedArrayBuffer          │
└──────────────────────────────────────────┘
         │
         ↓ Progressive completion
┌─────────────────┐
│  Main Thread    │  ← Reads buffer
│                 │  ← Colorizes
│                 │  ← Renders to canvas
└─────────────────┘
```

**Why this architecture:**
- Main thread never blocks
- Reference orbit computation isolated (can take time without affecting workers)
- Worker pool maximally utilized (only fast f64 perturbation)
- Quadtree logic stays with reference orbit (tightly coupled)

**Data flow:**

```rust
// Coordinator worker
fn on_viewport_change(viewport: Viewport) {
    // 1. Compute reference orbit (arbitrary precision)
    let ref_orbit = compute_reference_orbit(viewport.center);

    // 2. Adaptive quadtree subdivision
    let tiles = subdivide_adaptive(viewport, &ref_orbit);

    // 3. For each tile, if perturbation unstable:
    for tile in tiles {
        if is_perturbation_unstable(tile, &ref_orbit) {
            tile.reference = compute_reference_orbit(tile.center);
        }
    }

    // 4. Distribute to worker pool
    send_to_worker_pool(tiles);
}

// Worker pool
fn render_tile(tile: Tile) {
    // Fast f64 perturbation only
    for pixel in tile.pixels() {
        let delta = compute_perturbation_f64(pixel, &tile.reference);
        write_to_shared_buffer(pixel, delta);
    }
}
```

### RQ-7: High-Level Algorithm Design

**Describe a high-level algorithm implementing all requirements.**

**Answer: Four-phase architecture with coordinator pattern.**

**Overall Architecture:**

```
┌──────────────────────────────────────────────────────────────┐
│  MAIN THREAD (Leptos UI)                                     │
│  - User interaction (pan/zoom)                               │
│  - Canvas rendering (colorization)                           │
│  - Progressive display updates                               │
└───────────────┬──────────────────────────────────────────────┘
                │
                │ 1. postMessage(viewport, zoom_level)
                ↓
┌──────────────────────────────────────────────────────────────┐
│  COORDINATOR WORKER                                          │
│  - Reference orbit computation (arbitrary precision)         │
│  - Adaptive quadtree subdivision                             │
│  - Stability checking                                        │
│  - Task distribution                                         │
└───────────────┬──────────────────────────────────────────────┘
                │
                │ 2. Distribute tiles → Worker Pool
                ↓
┌──────────────────────────────────────────────────────────────┐
│  WORKER POOL (wasm-bindgen-rayon or manual workers)          │
│  Worker 1, 2, 3, ... N (CPU cores)                          │
│  - Perturbation calculations (f64)                           │
│  - Write directly to SharedArrayBuffer                       │
│  - Atomic tile completion markers                            │
└───────────────┬──────────────────────────────────────────────┘
                │
                │ 3. Progressive tile completion
                ↓
┌──────────────────────────────────────────────────────────────┐
│  MAIN THREAD                                                 │
│  - Polls SharedArrayBuffer for completed tiles               │
│  - Colorizes completed tiles                                 │
│  - Renders to canvas progressively                           │
└──────────────────────────────────────────────────────────────┘
```

**Phase 1: User Interaction → Cancellation**

```rust
ON_VIEWPORT_CHANGE(new_viewport):
    1. Set cancellation flag: cancel_token.cancel()
    2. Wait for workers to acknowledge (or timeout 100ms)
    3. Clear SharedArrayBuffer
    4. Reset progress indicators
    5. Send new render request to Coordinator
```

**Phase 2: Coordinator Worker - Reference & Subdivision**

```rust
ON_RENDER_REQUEST(viewport, zoom_level, cancel_token):
    // Step 1: Compute primary reference orbit
    let center = viewport.center()
    let max_iterations = calculate_iterations(zoom_level)

    // Arbitrary precision computation
    let reference_orbit = compute_reference_orbit_arbitrary_precision(
        center,
        max_iterations,
        cancel_token
    )

    if cancel_token.is_cancelled():
        return ABORT

    // Step 2: Adaptive quadtree subdivision
    let root_tile = Tile {
        bounds: viewport.bounds,
        reference: reference_orbit,
        level: 0
    }

    let tiles = adaptive_subdivide(root_tile, cancel_token)

    // Step 3: Send tiles to worker pool
    for tile in tiles:
        worker_pool.submit(tile)
```

**Adaptive Subdivision Algorithm:**

```rust
fn adaptive_subdivide(tile: Tile, cancel_token: CancelToken) -> Vec<Tile> {
    if cancel_token.is_cancelled():
        return vec![]

    // Base case: tile is small enough or stable
    if tile.width < MIN_TILE_SIZE || tile.level > MAX_DEPTH:
        return vec![tile]

    // Check stability at corners
    let corners = tile.get_four_corners()
    let perturbations = corners.map(|corner| {
        compute_perturbation_test(corner, &tile.reference)
    })

    // Stability criteria
    let is_stable = perturbations.all(|p| {
        p.error < STABILITY_THRESHOLD &&
        p.magnitude < MAGNITUDE_THRESHOLD
    })

    if is_stable:
        return vec![tile]  // Use this tile as-is
    else:
        // Subdivide into 4 quadrants
        let quadrants = tile.subdivide_into_four()

        // Each quadrant may need its own reference if very unstable
        for quad in quadrants:
            if needs_new_reference(quad, &tile.reference):
                quad.reference = compute_reference_orbit_arbitrary_precision(
                    quad.center(),
                    tile.max_iterations,
                    cancel_token
                )

        // Recursively subdivide each quadrant
        quadrants.flat_map(|q| adaptive_subdivide(q, cancel_token))
}
```

**Phase 3: Worker Pool - Perturbation Computation**

```rust
// Runs on each worker in parallel
fn worker_render_tile(
    tile: Tile,
    shared_buffer: SharedArrayBuffer,
    cancel_flag: AtomicBool
) {
    let tile_index = tile.index

    for pixel in tile.pixels():
        // Check cancellation every N pixels (tune for responsiveness)
        if pixel.index % 100 == 0 && cancel_flag.load(Ordering::Relaxed):
            return  // Abort this tile

        // Core perturbation calculation (all f64)
        let delta = compute_perturbation_f64(
            pixel.coordinate,
            &tile.reference,
            tile.max_iterations
        )

        // Write directly to shared buffer
        let buffer_offset = pixel.global_index
        shared_buffer.write_f64(buffer_offset, delta.escape_time)
        shared_buffer.write_f64(buffer_offset + 1, delta.magnitude)

    // Mark tile as complete (atomic operation)
    shared_buffer.mark_tile_complete(tile_index)

    // Notify main thread (optional, main can poll)
    postMessage({ type: 'tile_complete', tile: tile_index })
}
```

**Perturbation Calculation (f64):**

```rust
fn compute_perturbation_f64(
    c: ComplexF64,                   // Pixel coordinate (relative to reference)
    reference: &ReferenceOrbit,      // Pre-computed orbit
    max_iter: u32
) -> PerturbationResult {
    let delta_c = c  // Initial perturbation
    let mut delta_n = Complex64::zero()

    for n in 0..max_iter:
        let x_n = reference.orbit[n]  // Reference orbit point

        // Perturbation formula: Δₙ₊₁ = 2·Xₙ·Δₙ + Δₙ² + Δ₀
        delta_n = 2.0 * x_n * delta_n + delta_n * delta_n + delta_c

        // Check for escape
        if delta_n.norm_sqr() > 4.0:
            return PerturbationResult {
                escape_time: n as f64,
                magnitude: delta_n.norm()
            }

    PerturbationResult {
        escape_time: max_iter as f64,
        magnitude: 0.0
    }
}
```

**Phase 4: Main Thread - Progressive Rendering**

```rust
// Runs continuously via requestAnimationFrame
fn progressive_render_loop() {
    // Check SharedArrayBuffer for completed tiles
    let completed_tiles = shared_buffer.get_completed_tiles_since_last_check()

    if completed_tiles.is_empty():
        return  // Nothing new to render

    for tile_index in completed_tiles:
        // Read tile data from SharedArrayBuffer
        let tile_data = shared_buffer.read_tile(tile_index)

        // Colorize the tile
        let image_data = colorize_tile(tile_data, color_scheme)

        // Draw to canvas
        canvas_context.put_image_data(
            image_data,
            tile.x_offset,
            tile.y_offset
        )

    // Update progress indicator
    let progress = shared_buffer.get_completion_percentage()
    update_progress_ui(progress)

    // Schedule next check
    requestAnimationFrame(progressive_render_loop)
}
```

**Key Data Structures:**

```rust
struct ReferenceOrbit {
    center: ComplexArbitraryPrecision,
    orbit: Vec<Complex64>,  // Pre-computed reference points (stored as f64)
    max_iterations: u32,
}

struct Tile {
    bounds: Rectangle,      // In complex plane coordinates
    reference: ReferenceOrbit,
    level: u8,             // Quadtree depth
    index: u32,            // Unique tile ID
}

struct SharedArrayBuffer {
    data: Float64Array,                // Pixel values (escape time, magnitude)
    completion_flags: AtomicU8Array,   // Tile completion markers
    cancel_flag: AtomicBool,           // Global cancellation flag
}
```

**Optimizations:**

1. **Series Approximation:** Skip iterations for faster rendering
2. **Tile Ordering:** Render visible tiles first, then off-screen for smooth panning
3. **Caching:** Cache reference orbits for repeated renders at same location
4. **Memory Management:** Reuse SharedArrayBuffer between renders
5. **Worker Pool Size:** `navigator.hardwareConcurrency - 1` (leave one core for main thread)

**Requirements Satisfied:**
- ✓ Progressive rendering (Phase 4 polls continuously)
- ✓ Multi-core utilization (Worker pool + rayon)
- ✓ Responsive UI (cancellation via atomic flags)
- ✓ Extreme zoom (arbitrary precision for references, f64 for perturbations)
- ✓ Adaptive quadtree (Phase 2 subdivision)
- ✓ Separation of concerns (main=UI, coordinator=logic, workers=compute)

---

FURTHER NOTES:

- main thread waits for workers to be ready (if necessary) and can display a loading indicator while it's waiting
- WebAssembly 3.0 was released September 2025 (after knowledge cutoff). key features:
  - 64-bit address space
  - Multiple memories
  - Garbage collection
  - Typed references
  - Tail calls
  - Exception handling
  - Relaxed vector instructions
- perturbation theory: Multiple reference orbits required when a single reference orbit no longer yields numerically
  stable perturbations. When Multiple References Are Needed:
  1. Large deviation from reference pixel
    - Perturbation assumes pixel orbit stays close to reference
    - Spatial distance too large → perturbation series loses accurac
  2. Deep zooms with high magnification
    - Numerical precision becomes fragile
    - Round-off error accumulates
    - Subdividing into tiles with individual references restores stabilit
  3. Regions where orbit derivatives blow up
    - Near minibrots, filaments, sensitive dynamical areas
    - Correction terms amplify rapidly
    - Adjacent pixels diverge from shared reference too fas
  4. Long escape times
    - Many iterations before divergence
    - Small perturbations magnified
    - Multiple references reduce accumulated error

---

## Proposed Iteration Strategy

**Goal:** Build Web Workers + Perturbation Theory architecture incrementally, with each iteration delivering shippable, testable value.

**Constraints:**
- Each iteration must result in a working application (all tests pass, fully functional)
- Each iteration enables extensive user testing and experimentation
- No "global" final steps (tests/docs/integration happen per iteration)
- Each iteration builds on previous work

### Iteration 1: Workspace Architecture Foundation

**User Value:** Clean separation of concerns, foundation for workers

**What:**
- Restructure into Cargo workspace (fractalwonder-ui, fractalwonder-core, fractalwonder-compute)
- Move rendering logic to core crate (no DOM dependencies)
- Move UI components to ui crate (Leptos + DOM)
- Create empty compute crate (worker entry point skeleton)

**Validation:** App works identically to before, builds successfully, all tests pass

### Iteration 2: Progressive Rendering (Single-Threaded)

**User Value:** See fractal appear progressively instead of waiting for completion

**What:**
- Implement tile-based rendering (subdivide viewport into tiles)
- Implement SharedArrayBuffer for pixel data
- Main thread polls buffer and draws completed tiles to canvas
- Single-threaded computation (no workers yet)

**Validation:** User sees tiles appear one by one, progress indicator shows completion percentage

### Iteration 3: Multi-Core via Worker Pool

**User Value:** Faster renders through multi-core CPU utilization

**What:**
- Spawn Web Worker pool (one per CPU core)
- Distribute tiles to workers for parallel computation
- Workers write results to SharedArrayBuffer
- Main thread continues progressive canvas updates

**Validation:** CPU utilization increases, render time decreases, user can measure performance gains

### Iteration 4: Responsive Cancellation

**User Value:** Pan/zoom immediately stops render, UI never freezes

**What:**
- Add AtomicBool cancel flag to SharedArrayBuffer
- Workers poll cancel flag during computation
- Pan/zoom sets cancel flag and starts new render
- Track and display cancel/restart events

**Validation:** User pans/zooms during render, previous render stops immediately, new render starts

### Iteration 5: wasm-bindgen-rayon Parallelism

**User Value:** Even faster renders via work-stealing parallelism within workers

**What:**
- Add wasm-bindgen-rayon to compute workers
- Use par_iter for tile pixel computation
- Rayon thread pool within each worker

**Validation:** Benchmark shows measurable speedup, CPU utilization patterns change

### Iteration 6: Arbitrary Precision Reference Orbits

**User Value:** Foundation for deep zoom, demonstrates reference orbit computation

**What:**
- Create coordinator worker for reference orbit computation
- Compute single reference orbit using dashu (arbitrary precision)
- Pass reference orbit to compute workers
- Workers still use standard iteration (not perturbation yet)

**Validation:** Reference orbit computed correctly, can verify against known values, zoom deeper with arbitrary precision

### Iteration 7: Perturbation Theory Rendering

**User Value:** Zoom much deeper (10^100) while maintaining fast f64 calculations

**What:**
- Implement perturbation calculations (Δₙ₊₁ = 2·Xₙ·Δₙ + Δₙ² + Δ₀)
- Workers use perturbation instead of standard iteration
- Single reference orbit per viewport
- Convert pixel coordinates to delta from reference

**Validation:** User zooms to 10^50+ levels, renders remain fast, image quality maintained

### Iteration 8: Adaptive Quadtree + Multiple References

**User Value:** Extreme zoom (10^100+) with numerical stability

**What:**
- Implement stability checking for tiles
- Adaptive quadtree subdivision (subdivide unstable tiles)
- Compute new reference orbit for unstable tiles
- Coordinator distributes tiles with appropriate references

**Validation:** User zooms to 10^100+, image remains artifact-free, subdivision happens automatically

---

## Open Questions for Iteration Strategy

1. **MVP decision:** Should we implement iterations 1-5 first (workers without perturbation) and validate architecture before committing to perturbation complexity?

2. **Coordinator timing:** Could we delay coordinator worker (Iteration 6) and instead compute reference orbits on main thread initially, to simplify Iterations 1-5?

3. **Testing strategy:** What specific metrics/tests validate each iteration? (render time, zoom depth, responsiveness, CPU utilization)

4. **Rollback plan:** If perturbation proves too complex, can we ship iterations 1-5 as "multi-core progressive rendering" without deep zoom?

